import{_ as i,o as r,c as e,a as l}from"./app-xzMEyGNT.js";const a="/assets/system-design-79-9zyA3yGB.png",s="/assets/system-design-80-GfHTVhzu.png",n="/assets/system-design-82-HLhDd7La.png",t="/assets/system-design-83-vpyjZXuy.png",o="/assets/system-design-84-1H3uiHlL.png",g="/assets/system-design-85-W7EgqvlL.png",d="/assets/system-design-86-s-Ch8Mi_.png",h={},p=l('<h1 id="_9-设计网络爬虫" tabindex="-1"><a class="header-anchor" href="#_9-设计网络爬虫" aria-hidden="true">#</a> 9. 设计网络爬虫</h1><p>这是一个经典的系统设计问题：设计一个网络爬虫（Web Crawler）。</p><p>网络爬虫（又称机器人）用于在互联网上发现新内容或更新内容，例如文章、视频、PDF 等。</p><figure><img src="'+a+'" alt="web-crawler-example" tabindex="0" loading="lazy"><figcaption>web-crawler-example</figcaption></figure><p>使用场景：</p><ul><li><strong>搜索引擎索引</strong>：用于创建搜索引擎的本地索引，例如 Google 的 Googlebot。</li><li><strong>网页归档</strong>：从网络中收集数据并保存以备将来使用。</li><li><strong>网络挖掘</strong>：也可用于数据挖掘。例如，为交易公司寻找股东大会等重要信息。</li><li><strong>网络监控</strong>：监控网络上的版权侵权、公司内部信息泄露等。</li></ul><p>网络爬虫的复杂性取决于目标规模，它可以非常简单（如学生项目），也可以是一个需要专门团队维护的多年项目。</p><h2 id="第一步-理解问题并确定设计范围" tabindex="-1"><a class="header-anchor" href="#第一步-理解问题并确定设计范围" aria-hidden="true">#</a> 第一步：理解问题并确定设计范围</h2><h3 id="高层次的工作原理" tabindex="-1"><a class="header-anchor" href="#高层次的工作原理" aria-hidden="true">#</a> 高层次的工作原理</h3><ol><li>给定一组 URL，下载这些 URL 指向的所有页面。</li><li>从网页中提取 URL。</li><li>将新提取的 URL 添加到待遍历列表中。</li></ol><p>尽管实际的网络爬虫远比这个复杂，但核心流程可以总结为以上内容。</p><h3 id="功能性需求" tabindex="-1"><a class="header-anchor" href="#功能性需求" aria-hidden="true">#</a> 功能性需求</h3><p>在设计过程中，你需要明确面试官希望支持的具体功能。例如：</p><ul><li><strong>候选人</strong>：网络爬虫的主要用途是什么？搜索引擎索引还是数据挖掘？ <strong>面试官</strong>：搜索引擎索引。</li><li><strong>候选人</strong>：每月需要抓取多少网页？ <strong>面试官</strong>：10 亿个。</li><li><strong>候选人</strong>：支持哪些内容类型？HTML、PDF、图片？ <strong>面试官</strong>：仅 HTML。</li><li><strong>候选人</strong>：是否需要考虑新增或修改的内容？ <strong>面试官</strong>：是。</li><li><strong>候选人</strong>：是否需要持久化抓取的网页？ <strong>面试官</strong>：是，保存 5 年。</li><li><strong>候选人</strong>：如何处理重复内容的页面？ <strong>面试官</strong>：忽略。</li></ul><p>通过类似的对话，你可以澄清设计范围和假设，这对于即使是简单项目也很重要。</p><h3 id="非功能性需求" tabindex="-1"><a class="header-anchor" href="#非功能性需求" aria-hidden="true">#</a> 非功能性需求</h3><ul><li><strong>鲁棒性（Robust）</strong>：能够处理异常情况，例如不良 HTML、无限循环、服务器崩溃等。</li><li><strong>友好性（Polite）</strong>：避免在短时间内向同一服务器发送过多请求。</li><li><strong>可扩展性（Extensibility）</strong>：便于未来添加对新内容类型的支持，例如图片。</li></ul><h3 id="粗略估算" tabindex="-1"><a class="header-anchor" href="#粗略估算" aria-hidden="true">#</a> 粗略估算</h3><ul><li>每月 10 亿个页面 → 每秒约 400 页。</li><li>峰值 QPS = 800 页每秒。</li><li>平均网页大小为 500 KB → 每月 500 TB → 5 年共 30 PB。</li></ul><h2 id="第二步-提出高层设计并获得认可" tabindex="-1"><a class="header-anchor" href="#第二步-提出高层设计并获得认可" aria-hidden="true">#</a> 第二步：提出高层设计并获得认可</h2><p>网络爬虫的工作流如下：</p><figure><img src="'+s+'" alt="high-level-design" tabindex="0" loading="lazy"><figcaption>high-level-design</figcaption></figure><ol><li><p><strong>种子 URL 提供初始输入</strong>：系统从一组初始 URL 开始（通常称为种子 URL），这些 URL 是爬虫的起点。</p></li><li><p><strong>URL 队列管理</strong>：所有待抓取的 URL 存储在一个队列中。队列按照一定的规则（如 FIFO 或优先级）安排爬取顺序。</p></li><li><p><strong>HTML 下载器抓取内容</strong>：从 URL 队列中获取下一个待抓取的 URL，并使用 HTML 下载器下载网页内容。</p></li><li><p><strong>DNS 解析</strong>：在下载 HTML 页面之前，爬虫需要通过 DNS 解析将 URL 转换为服务器的 IP 地址。</p></li><li><p><strong>内容解析与验证</strong>：下载的网页内容会通过解析器进行验证，确保其完整性和可用性。如果内容损坏或不符合要求，将丢弃。</p></li><li><p><strong>内容去重</strong>：通过比较网页哈希值，确保网页内容和已处理的页面不重复。</p></li><li><p><strong>内容存储</strong>：验证后的网页内容会被存储到数据库或文件系统中，供后续分析或检索使用，大部分内容存储在磁盘上，热门内容存储在内存中以便快速检索。</p></li><li><p><strong>URL 提取与过滤</strong>：从 HTML 文档中提取所有超链接，将有效的链接加入待抓取的 URL 队列。</p><ul><li><p><strong>URL 过滤器</strong>：排除无效或不需要的链接，例如重复的 URL、不支持的内容类型、或在黑名单中的链接。</p></li><li><p><strong>URL 去重</strong>：使用哈希表或布隆过滤器检查提取的链接是否已被访问过，以避免重复抓取。</p></li><li><p><strong>URL 存储</strong>：已访问的 URL 会被存储到数据库中，用于 URL 去重，避免重复遍历。</p></li></ul></li></ol><h2 id="第三步-深入设计" tabindex="-1"><a class="header-anchor" href="#第三步-深入设计" aria-hidden="true">#</a> 第三步：深入设计</h2><p>现在让我们探讨一下网络爬虫中一些最重要的机制：</p><ul><li>DFS vs. BFS</li><li>URL 队列设计</li><li>HTML 下载器</li><li>鲁棒性</li><li>可扩展性</li><li>检测与规避问题内容</li></ul><h3 id="dfs-vs-bfs" tabindex="-1"><a class="header-anchor" href="#dfs-vs-bfs" aria-hidden="true">#</a> DFS vs. BFS</h3><p>互联网是一个有向图，网页中的链接是指向其他页面（节点）的边。</p><p>常见的两种遍历方法是深度优先搜索（DFS）和广度优先搜索（BFS）。</p><ul><li>DFS 通常不是一个好选择，因为遍历深度可能非常大。</li><li>BFS 更适合，使用 FIFO 队列按遇到 URL 的顺序遍历。</li></ul><p>不过，传统 BFS 存在两个问题：</p><ul><li>大部分链接是指向同一域名的回链（例如 <code>wikipedia.com/page -&gt; wikipedia.com</code>），短时间内大量访问同一服务器会“不礼貌”。</li><li>标准 BFS 没有考虑 URL 的优先级。</li></ul><h3 id="url-队列" tabindex="-1"><a class="header-anchor" href="#url-队列" aria-hidden="true">#</a> URL 队列</h3><p>URL 队列解决了上述问题，确保了<strong>优先级</strong>和<strong>友好性</strong>。</p><h3 id="友好性" tabindex="-1"><a class="header-anchor" href="#友好性" aria-hidden="true">#</a> 友好性</h3><p>为了避免在短时间内对同一主机发送过多请求，爬虫为每个主机维护单独的下载队列，并在两次请求之间设置延迟。</p><figure><img src="'+n+'" alt="download-queue" tabindex="0" loading="lazy"><figcaption>download-queue</figcaption></figure><ul><li>队列路由器：确保每个队列包含来自同一主机的 URL。</li><li>映射表：将主机映射到相应队列。</li></ul><figure><img src="'+t+'" alt="mapping-table" tabindex="0" loading="lazy"><figcaption>mapping-table</figcaption></figure><ul><li>FIFO 队列：保存来自同一主机的 URL。</li><li>队列选择器：将工作线程分配给 FIFO 队列，每个线程只从一个 FIFO 队列中下载 URL。队列选择器负责分配哪个工作线程处理哪个队列。。</li><li>工作线程：工作线程从同一 FIFO 队列逐个下载网页，并在两次下载之间设置延迟。</li></ul><h3 id="优先级" tabindex="-1"><a class="header-anchor" href="#优先级" aria-hidden="true">#</a> 优先级</h3><p>根据页面的重要性（例如 PageRank、网页流量、更新频率）为 URL 进行优先排序。</p><p>优先级排序器（Prioritizer）管理每个 URL 的优先级：</p><figure><img src="'+o+`" alt="prioritizer" tabindex="0" loading="lazy"><figcaption>prioritizer</figcaption></figure><ul><li>将 URL 作为输入并计算优先级</li><li>每个队列具有不同的优先级，URL 根据其优先级放入相应的队列。</li><li>队列选择器随机选择队列，并偏向高优先级队列。</li></ul><h3 id="内容更新" tabindex="-1"><a class="header-anchor" href="#内容更新" aria-hidden="true">#</a> 内容更新</h3><p>网页不断更新，因此还需要定期重新抓取更新的内容。</p><p>可以根据网页的更新历史选择重新抓取，还可以优先重新抓取更新较多的重要页面。</p><h3 id="内容存储" tabindex="-1"><a class="header-anchor" href="#内容存储" aria-hidden="true">#</a> 内容存储</h3><p>在实际应用中，URL 队列可能包含数百万条 URL。全部存储在内存中不可行，而存储在磁盘上速度又太慢。<br> 采用<strong>混合存储</strong>方案：大部分 URL 存储在磁盘上，当前处理的 URL 缓存在内存中，并定期刷新到磁盘。</p><h3 id="url-队列存储" tabindex="-1"><a class="header-anchor" href="#url-队列存储" aria-hidden="true">#</a> URL 队列存储</h3><p>在现实世界中，URL 队列中的 URL 可能有数百万个，将所有内容都放在内存中是不可行的。 但将其放在磁盘上也很慢，并且可能导致抓取逻辑出现瓶颈。</p><p>可以采用混合存储的方法，其中大多数 URL 都存在磁盘上，但在内存中维护一个缓冲区，其中包含当前正在处理的 URL，定期将其刷新到磁盘。</p><h3 id="html-下载器" tabindex="-1"><a class="header-anchor" href="#html-下载器" aria-hidden="true">#</a> HTML 下载器</h3><p>HTML 下载器使用 HTTP 协议从网络下载 HTML 页面。</p><p>还需要记住的一个协议是 Robots 排除协议，它是一个可在网站上使用的 <code>robots.txt</code> 文件，网站所有者使用它与网络爬虫进行通信，用于传达哪些网页可以遍历以及哪些网页应该跳过。</p><p><code>robots.txt</code> 文件示例：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>User-agent：Googlebot
Disallow：/creatorhub/\\*
Disallow：/rss/people/\\*/reviews
Disallow：/gp/pdp/rss/\\*/reviews
Disallow：/gp/cdp/member-reviews/
Disallow：/gp/aw/cr/
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>需要按照该文件的规则，避免抓取其中指定跳过的页面，可以缓存该文件以避免一直重复下载。</p><h3 id="性能优化" tabindex="-1"><a class="header-anchor" href="#性能优化" aria-hidden="true">#</a> 性能优化</h3><p>我们可以考虑对 HTML 下载器进行一些性能优化。</p><ul><li><strong>分布式抓取</strong>：我们可以将抓取作业并行化到多台运行多个线程的机器上，以便更有效地进行抓取。</li></ul><figure><img src="`+g+'" alt="distributed-crawl" tabindex="0" loading="lazy"><figcaption>distributed-crawl</figcaption></figure><ul><li><strong>缓存 DNS 解析器</strong>：我们可以维护自己的 DNS 缓存，以避免一直向 DNS 解析器发出请求，这可能会很昂贵，并定期更新缓存。</li><li><strong>位置</strong>：我们可以根据地理位置分配抓取作业。当抓取器在物理上更靠近网站服务器时，延迟会更低。</li><li><strong>短超时</strong>：我们需要添加超时，以防服务器响应速度超过给定阈值。否则，我们的抓取器可能会花费大量时间等待永远不会出现的页面。</li></ul><h3 id="鲁棒性" tabindex="-1"><a class="header-anchor" href="#鲁棒性" aria-hidden="true">#</a> 鲁棒性</h3><p>实现系统稳健性的一些方法：</p><ul><li><strong>一致性哈希</strong>：为了方便扩展和缩减工作节点/爬虫等资源，我们可以在负载均衡时使用一致性哈希。</li><li><strong>保存爬取状态和数据</strong>：在服务器发生崩溃时，可以将中间结果存储到磁盘上，这样其他工作节点能够从上一次中断的地方继续任务。</li><li><strong>异常处理</strong>：我们需要优雅地处理异常，避免因错误导致服务器崩溃。在足够大的系统中，异常是不可避免的。</li><li><strong>数据验证</strong>：进行数据验证是防止系统错误的重要安全措施。</li></ul><h3 id="可扩展性" tabindex="-1"><a class="header-anchor" href="#可扩展性" aria-hidden="true">#</a> 可扩展性</h3><p>为了支持未来的新内容类型，我们需要确保爬虫具备良好的可扩展性：</p><figure><img src="'+d+'" alt="extendable-crawler" tabindex="0" loading="lazy"><figcaption>extendable-crawler</figcaption></figure><p>示例扩展：</p><ul><li>增加 <strong>PNG 下载器</strong> 用于爬取 PNG 图片。</li><li>增加 <strong>Web 监控器</strong> 用于监控版权侵权行为。</li></ul><h3 id="识别并避免问题内容" tabindex="-1"><a class="header-anchor" href="#识别并避免问题内容" aria-hidden="true">#</a> 识别并避免问题内容</h3><ul><li><strong>冗余内容</strong>：互联网上约 30% 的网页是重复内容。通过哈希值/校验和避免重复处理。</li><li><strong>爬虫陷阱</strong>：一些网页可能会导致爬虫进入无限循环，例如极深的目录结构。我们可以通过设置 URL 的最大长度来避免此类问题。此外，可以引入人工干预的功能，将这些陷阱网页列入黑名单。</li><li><strong>数据噪声</strong>：过滤掉广告、代码片段、垃圾内容等无价值内容。</li></ul><h2 id="第四步-总结" tabindex="-1"><a class="header-anchor" href="#第四步-总结" aria-hidden="true">#</a> 第四步：总结</h2><p>一个优秀的爬虫的特点：可扩展性、礼貌、可扩展性、鲁棒性。</p><p>其他相关的讨论点：</p><ul><li><strong>服务器端渲染</strong>：许多网站动态生成 HTML。如果我们在没有生成 HTML 的情况下直接解析，它将错过网站上的信息。为了解决这个问题，我们在解析页面之前先进行服务器端渲染。</li><li><strong>过滤不需要的页面</strong>：反垃圾邮件组件有助于过滤低质量页面。</li><li><strong>数据库复制和分片</strong>：这些是提高数据层可用性、可扩展性和可靠性的有效技术。</li><li><strong>水平扩展</strong>：关键是保持服务器无状态，从而使每种类型的服务器、工作线程、爬虫等都能水平扩展。</li><li><strong>可用性、一致性、可靠性</strong>：这些是任何大型系统成功的核心概念。</li><li><strong>数据分析</strong>：我们可能还需要收集和分析数据，以进一步优化我们的系统。</li></ul>',78),c=[p];function u(f,L){return r(),e("div",null,c)}const m=i(h,[["render",u],["__file","29_design_a_web_crawler.html.vue"]]);export{m as default};
