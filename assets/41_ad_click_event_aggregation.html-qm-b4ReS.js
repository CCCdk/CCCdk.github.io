import{_ as a,o as d,c as l,f as i,a as e,b as t}from"./app-UUuqQ2DY.js";const s="/assets/system-design-284-ShVGWr-Q.png",n="/assets/system-design-285-6Ajuw4_G.png",r="/assets/system-design-286-IQo5stg4.png",p="/assets/system-design-287-S3lFa21P.png",o="/assets/system-design-288-8vIIUIl-.png",c="/assets/system-design-289-blgj-sf8.png",g="/assets/system-design-290-s_LmpWzw.png",u="/assets/system-design-291-WqYzVDl3.png",h="/assets/system-design-292-xBefu062.png",m="/assets/system-design-293-w0Y636H5.png",f="/assets/system-design-294-MamWtXcl.png",_="/assets/system-design-295-BcfWUInJ.png",b="/assets/system-design-296-9KdZYiOc.png",y="/assets/system-design-297-lF1ZE5GT.png",x="/assets/system-design-298-DgLoyWwe.png",v="/assets/system-design-299-FrLm9afP.png",k="/assets/system-design-300-V-GfP26i.png",z="/assets/system-design-301-C8xjEtdG.png",w="/assets/system-design-302-u_BQ7pYk.png",A="/assets/system-design-303-v5b3OldO.png",S="/assets/system-design-304-sr2hfnAQ.png",P="/assets/system-design-305-drRMFLnN.png",B="/assets/system-design-306-kdurs7EG.png",M="/assets/system-design-307-9pZzbN7v.png",N="/assets/system-design-308-yHFU_Hzu.png",G="/assets/system-design-310-Pc-jKCwb.png",T="/assets/system-design-311-_qdOkaYL.png",C="/assets/system-design-312-q_elCEbb.png",R={},U=e('<h1 id="_21-设计广告点击事件聚合" tabindex="-1"><a class="header-anchor" href="#_21-设计广告点击事件聚合" aria-hidden="true">#</a> 21. 设计广告点击事件聚合</h1><p>随着 Facebook、YouTube、TikTok 等平台的兴起，数字广告行业变得越来越庞大。</p><p>因此，追踪广告点击事件变得非常重要。本章将探讨如何在 Facebook/Google 规模上设计一个广告点击事件聚合系统。</p><p>数字广告有一个叫做实时竞价（RTB）的过程，在这个过程中，数字广告库存被买卖：</p><figure><img src="'+s+`" alt="digital-advertising-example" tabindex="0" loading="lazy"><figcaption>digital-advertising-example</figcaption></figure><p>RTB 的速度非常重要，因为它通常发生在一秒钟之内。 数据准确性同样至关重要，因为它影响广告主的支付金额。</p><p>基于广告点击事件的聚合，广告主可以做出一些决策，例如调整目标受众和关键词。</p><h2 id="第一步-理解问题并确定设计范围" tabindex="-1"><a class="header-anchor" href="#第一步-理解问题并确定设计范围" aria-hidden="true">#</a> 第一步：理解问题并确定设计范围</h2><ul><li><strong>候选人</strong>: 输入数据的格式是什么？</li><li><strong>面试官</strong>: 每天 10 亿次广告点击，共有 200 万个广告。广告点击事件的数量每年增长 30%。</li><li><strong>候选人</strong>: 系统需要支持哪些最重要的查询？</li><li><strong>面试官</strong>: 需要考虑的主要查询包括： <ul><li>返回广告 X 在过去 Y 分钟内的点击次数</li><li>返回过去 1 分钟内点击次数最多的前 100 个广告。两个参数应可配置。聚合每分钟进行一次。</li><li>支持按<code>ip</code>、<code>user_id</code>、<code>country</code>等属性进行数据过滤。</li></ul></li><li><strong>候选人</strong>: 我们需要担心边缘情况吗？我能想到的一些边缘情况包括： <ul><li>可能会有事件到达比预期晚</li><li>可能会有重复事件</li><li>系统的不同部分可能会出现故障，因此我们需要考虑系统恢复</li></ul></li><li><strong>面试官</strong>: 这是一个很好的列表，请考虑这些情况。</li><li><strong>候选人</strong>: 延迟要求是什么？</li><li><strong>面试官</strong>: 广告点击聚合的端到端延迟为几分钟。对于 RTB，则要求低于一秒。广告点击聚合的延迟是可以接受的，因为这些数据通常用于计费和报告。</li></ul><h3 id="功能要求" tabindex="-1"><a class="header-anchor" href="#功能要求" aria-hidden="true">#</a> 功能要求</h3><ul><li>聚合广告<code>ad_id</code>在过去 Y 分钟内的点击次数</li><li>每分钟返回点击次数最多的前 100 个<code>ad_id</code></li><li>支持按不同属性进行聚合过滤</li><li>数据集的体量为 Facebook 或 Google 级别</li></ul><h3 id="非功能要求" tabindex="-1"><a class="header-anchor" href="#非功能要求" aria-hidden="true">#</a> 非功能要求</h3><ul><li>聚合结果的正确性非常重要，因为它用于 RTB 和广告计费</li><li>正确处理延迟或重复事件</li><li>系统的鲁棒性——系统应能抵御部分故障</li><li>延迟——最多几分钟的端到端延迟</li></ul><h3 id="粗略估算" tabindex="-1"><a class="header-anchor" href="#粗略估算" aria-hidden="true">#</a> 粗略估算</h3><ul><li>10 亿日活跃用户（DAU）</li><li>假设每个用户每天点击 1 个广告 -&gt; 每天 10 亿次广告点击</li><li>广告点击 QPS = 10,000</li><li>峰值 QPS 是正常的 5 倍 = 50,000</li><li>单个广告点击占用 0.1KB 存储。每天存储需求为 100GB</li><li>每月存储需求 = 3TB</li></ul><h2 id="第二步-提出高层设计并获得认同" tabindex="-1"><a class="header-anchor" href="#第二步-提出高层设计并获得认同" aria-hidden="true">#</a> 第二步：提出高层设计并获得认同</h2><p>本节我们将讨论查询 API 设计、数据模型和高层设计。</p><h3 id="查询-api-设计" tabindex="-1"><a class="header-anchor" href="#查询-api-设计" aria-hidden="true">#</a> 查询 API 设计</h3><p>API 是客户端与服务器之间的契约。在我们的案例中，客户端是仪表盘用户——数据科学家/分析师、广告主等。</p><p>以下是我们的功能需求：</p><ul><li>聚合广告<code>ad_id</code>在过去 Y 分钟内的点击次数</li><li>返回过去 M 分钟内点击次数最多的前 N 个<code>ad_id</code></li><li>支持按不同属性进行聚合过滤</li></ul><p>我们需要两个端点来实现这些需求。可以通过查询参数进行过滤。</p><p>聚合广告<code>ad_id</code>在过去 M 分钟内的点击次数：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>GET /v1/ads/{:ad_id}/aggregated_count
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>查询参数：</p><ul><li>from - 起始分钟。默认值为当前时间前 1 分钟</li><li>to - 结束分钟。默认值为当前时间</li><li>filter - 用于不同过滤策略的标识符。例如，001 表示“非美国点击”。</li></ul><p>响应：</p><ul><li>ad_id - 广告标识符</li><li>count - 起始和结束分钟之间的聚合次数</li></ul><p>返回过去 M 分钟内点击次数最多的前 N 个广告：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>GET /v1/ads/popular_ads
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>查询参数：</p><ul><li>count - 前 N 个点击最多的广告</li><li>window - 聚合窗口大小（以分钟为单位）</li><li>filter - 用于不同过滤策略的标识符</li></ul><p>响应：</p><ul><li>广告 ID 列表</li></ul><h3 id="数据模型" tabindex="-1"><a class="header-anchor" href="#数据模型" aria-hidden="true">#</a> 数据模型</h3><p>在我们的系统中，我们有原始数据和聚合数据。</p><p>原始数据如下所示：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>[AdClickEvent] ad001, 2021-01-01 00:00:01, user 1, 207.148.22.22, USA
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>这是一个结构化的示例：</p>`,39),I=e("<table><thead><tr><th>ad_id</th><th>click_timestamp</th><th>user</th><th>ip</th><th>country</th></tr></thead><tbody><tr><td>ad001</td><td>2021-01-01 00:00:01</td><td>user1</td><td>207.148.22.22</td><td>USA</td></tr><tr><td>ad001</td><td>2021-01-01 00:00:02</td><td>user1</td><td>207.148.22.22</td><td>USA</td></tr><tr><td>ad002</td><td>2021-01-01 00:00:02</td><td>user2</td><td>209.153.56.11</td><td>USA</td></tr></tbody></table><p>这是聚合后的版本：</p>",2),F=e("<table><thead><tr><th>ad_id</th><th>click_minute</th><th>filter_id</th><th>count</th></tr></thead><tbody><tr><td>ad001</td><td>202101010000</td><td>0012</td><td>2</td></tr><tr><td>ad001</td><td>202101010000</td><td>0023</td><td>3</td></tr><tr><td>ad001</td><td>202101010001</td><td>0012</td><td>1</td></tr><tr><td>ad001</td><td>202101010001</td><td>0023</td><td>6</td></tr></tbody></table><p><code>filter_id</code>帮助我们实现过滤需求。</p>",2),K=e("<table><thead><tr><th>filter_id</th><th>region</th><th>IP</th><th>user_id</th></tr></thead><tbody><tr><td>0012</td><td>US</td><td>*</td><td>_</td></tr><tr><td>0013</td><td>_</td><td>123.1.2.3</td><td>_</td></tr></tbody></table><p>为了支持快速返回过去 M 分钟内点击次数最多的前 N 个广告，我们还将维护以下结构：</p>",2),O=e('<table><thead><tr><th>most_clicked_ads</th><th></th><th></th></tr></thead><tbody><tr><td>window_size</td><td>integer</td><td>聚合窗口大小（M）以分钟为单位</td></tr><tr><td>update_time_minute</td><td>timestamp</td><td>上次更新时间戳（以1分钟为单位）</td></tr><tr><td>most_clicked_ads</td><td>array</td><td>广告ID的JSON格式列表</td></tr></tbody></table><p>存储原始数据和存储聚合数据之间有哪些利弊？</p><ul><li>原始数据允许使用完整的数据集，并支持数据过滤和重新计算</li><li>聚合数据允许我们拥有较小的数据集，并且查询更快</li><li>原始数据意味着需要更大的数据存储，并且查询较慢</li><li>聚合数据是衍生数据，因此存在一定的数据丢失</li></ul><p>在我们的设计中，我们将结合这两种方法：</p><ul><li>保留原始数据对于调试很有帮助。如果聚合出现了问题，我们可以发现错误并回填数据。</li><li>聚合数据也应该存储，以便更快的查询性能。</li><li>原始数据可以存储在冷存储中，以避免额外的存储成本。</li></ul><p>在选择数据库时，有几个因素需要考虑：</p><ul><li>数据的类型是什么？是关系型、文档型还是二进制大对象（BLOB）？</li><li>工作负载是以读取为主、写入为主还是两者都有？</li><li>是否需要事务支持？</li><li>查询是否依赖于 OLAP 函数，如 SUM 和 COUNT？</li></ul><p>对于原始数据，我们可以看到平均 QPS 为 10k，峰值 QPS 为 50k，因此系统是写入密集型的。 另一方面，读取流量较低，因为原始数据主要作为备份，以防出现问题。</p><p>关系型数据库能够完成这个任务，但扩展写入操作会比较有挑战。 另一种选择是使用 Cassandra 或 InfluxDB，它们对重负载写入有更好的原生支持。</p><p>另一个选择是使用 Amazon S3 和列式数据格式（如 ORC、Parquet 或 AVRO）。由于这种设置不太熟悉，我们将选择 Cassandra。</p><p>对于聚合数据，工作负载既有读取也有写入，因为聚合数据经常被用来为仪表盘和警报提供支持。 它也是写入密集型的，因为数据是每分钟聚合并写入的。因此，我们在这里也会使用相同的数据存储（Cassandra）。</p><h3 id="高层设计" tabindex="-1"><a class="header-anchor" href="#高层设计" aria-hidden="true">#</a> 高层设计</h3><p>这是我们系统的架构：</p><figure><img src="'+n+'" alt="high-level-design-1" tabindex="0" loading="lazy"><figcaption>high-level-design-1</figcaption></figure><p>数据流作为一个无界数据流，输入和输出都如此。</p><p>为了避免同步的汇聚点（即消费者崩溃可能导致整个系统停滞），我们将利用异步处理，使用消息队列（Kafka）解耦消费者和生产者。</p><figure><img src="'+r+'" alt="high-level-design-2" tabindex="0" loading="lazy"><figcaption>high-level-design-2</figcaption></figure><p>第一个消息队列存储广告点击事件数据：</p>',18),W=t("table",null,[t("thead",null,[t("tr",null,[t("th",null,"ad_id"),t("th",null,"click_timestamp"),t("th",null,"user_id"),t("th",null,"ip"),t("th",null,"country")])])],-1),D=t("p",null,"第二个消息队列包含每分钟聚合的广告点击计数：",-1),L=t("table",null,[t("thead",null,[t("tr",null,[t("th",null,"ad_id"),t("th",null,"click_minute"),t("th",null,"count")])])],-1),Y=t("p",null,"以及每分钟聚合的点击次数",-1),E=t("p",null,"最多的前 N 个广告：",-1),Q=e('<table><thead><tr><th>update_time_minute</th><th>most_clicked_ads</th></tr></thead></table><p>第二个消息队列的存在是为了实现端到端精确一次的原子提交语义：</p><figure><img src="'+p+'" alt="atomic-commit" tabindex="0" loading="lazy"><figcaption>atomic-commit</figcaption></figure><p>对于聚合服务，使用 MapReduce 框架是一个不错的选择：</p><figure><img src="'+o+'" alt="ad-count-map-reduce" tabindex="0" loading="lazy"><figcaption>ad-count-map-reduce</figcaption></figure><figure><img src="'+c+'" alt="top-100-map-reduce" tabindex="0" loading="lazy"><figcaption>top-100-map-reduce</figcaption></figure><p>每个节点负责一个单独的任务，并将处理结果发送给下游节点。</p><p>Map 节点负责从数据源读取数据，然后进行过滤和转换。</p><p>例如，Map 节点可以根据<code>ad_id</code>将数据分配到不同的聚合节点：</p><figure><img src="'+g+'" alt="map-node" tabindex="0" loading="lazy"><figcaption>map-node</figcaption></figure><p>或者，我们可以将广告分布到 Kafka 分区中，让聚合节点直接在消费者组中进行订阅。 然而，Map 节点可以帮助我们在后续处理之前进行数据清洗或转换。</p><p>另一个原因是我们可能无法控制数据的生产方式， 因此与同一<code>ad_id</code>相关的事件可能会被发送到不同的分区。</p><p>聚合节点每分钟在内存中统计广告点击事件，按<code>ad_id</code>进行聚合。</p><p>Reduce 节点收集来自聚合节点的聚合结果，并生成最终结果：</p><figure><img src="'+u+'" alt="reduce-node" tabindex="0" loading="lazy"><figcaption>reduce-node</figcaption></figure><p>此 DAG 模型使用了 MapReduce 范式。它通过并行分布式计算将大数据转换为常规大小的数据。</p><p>在 DAG 模型中，临时数据存储在内存中，不同节点之间使用 TCP 或共享内存进行通信。</p><p>现在让我们探索一下这个模型如何帮助我们实现各种用例。</p><p>用例 1 - 聚合点击次数：</p><figure><img src="'+h+'" alt="use-case-1" tabindex="0" loading="lazy"><figcaption>use-case-1</figcaption></figure><ul><li>广告通过<code>ad_id % 3</code>进行分区</li></ul><p>用例 2 - 返回点击次数最多的前 N 个广告：</p><figure><img src="'+m+'" alt="use-case-2" tabindex="0" loading="lazy"><figcaption>use-case-2</figcaption></figure><ul><li>在这个案例中，我们聚合了前 3 个广告，但这可以轻松扩展到前 N 个广告</li><li>每个节点维护一个堆数据结构，以便快速检索前 N 个广告</li></ul><p>用例 3 - 数据过滤： 为了支持快速的数据过滤，我们可以预定义过滤标准，并基于此进行预聚合：</p>',25),V=e('<table><thead><tr><th>ad_id</th><th>click_minute</th><th>country</th><th>count</th></tr></thead><tbody><tr><td>ad001</td><td>202101010001</td><td>USA</td><td>100</td></tr><tr><td>ad001</td><td>202101010001</td><td>GPB</td><td>200</td></tr><tr><td>ad001</td><td>202101010001</td><td>others</td><td>3000</td></tr><tr><td>ad002</td><td>202101010001</td><td>USA</td><td>10</td></tr><tr><td>ad002</td><td>202101010001</td><td>GPB</td><td>25</td></tr><tr><td>ad002</td><td>202101010001</td><td>others</td><td>12</td></tr></tbody></table><p>这种技术称为星型模式（star schema），广泛应用于数据仓库中。 过滤字段被称为维度。</p><p>这种方法的好处包括：</p><ul><li>易于理解和构建</li><li>当前的聚合服务可以重用，以创建更多的维度</li><li>基于过滤条件访问数据非常快速，因为结果是预计算的</li></ul><p>这种方法的一个限制是，它会创建更多的分区和记录，尤其是在有许多过滤条件时。</p><h2 id="第三步-设计深入分析" tabindex="-1"><a class="header-anchor" href="#第三步-设计深入分析" aria-hidden="true">#</a> 第三步：设计深入分析</h2><p>让我们深入探讨一些更有趣的话题。</p><h3 id="流处理-vs-批处理" tabindex="-1"><a class="header-anchor" href="#流处理-vs-批处理" aria-hidden="true">#</a> 流处理 vs. 批处理</h3><p>我们提出的高层架构是一种流处理系统。 以下是三种系统类型的比较：</p><table><thead><tr><th></th><th>在线系统（服务）</th><th>批处理系统（离线系统）</th><th>流处理系统（近实时系统）</th></tr></thead><tbody><tr><td>响应性</td><td>快速响应客户端</td><td>不需要响应客户端</td><td>不需要响应客户端</td></tr><tr><td>输入</td><td>用户请求</td><td>有限大小的有界输入，大量数据</td><td>输入没有边界（无限流）</td></tr><tr><td>输出</td><td>客户端响应</td><td>物化视图、聚合指标等</td><td>物化视图、聚合指标等</td></tr><tr><td>性能衡量</td><td>可用性、延迟</td><td>吞吐量</td><td>吞吐量、延迟</td></tr><tr><td>示例</td><td>在线购物</td><td>MapReduce</td><td>Flink [13]</td></tr></tbody></table><p>在我们的设计中，我们结合使用了批处理和流处理。</p><p>我们使用流处理来处理到达的数据，并生成近实时的聚合结果。 另一方面，我们使用批处理来进行历史数据备份。</p><p>包含两个处理路径——批处理和流处理的系统称为 Lambda 架构。 其缺点是，你需要维护两个不同代码库的处理路径。</p><p>Kappa 架构是一种替代架构，它将批处理和流处理合并到一个处理路径中。 关键思想是使用单一的流处理引擎。</p><p>Lambda 架构：</p><figure><img src="'+f+'" alt="lambda-architecture" tabindex="0" loading="lazy"><figcaption>lambda-architecture</figcaption></figure><p>Kappa 架构：</p><figure><img src="'+_+'" alt="kappa-architecture" tabindex="0" loading="lazy"><figcaption>kappa-architecture</figcaption></figure><p>我们的高层设计使用了 Kappa 架构，因为历史数据的重新处理也会通过聚合服务。</p><p>每当由于聚合逻辑中的重大错误需要重新计算聚合数据时，我们可以从存储的原始数据中重新计算聚合。</p><ul><li>重新计算服务从原始存储中检索数据。这是一个批处理作业。</li><li>检索到的数据被发送到专门的聚合服务，这样实时处理聚合服务就不会受到影响。</li><li>聚合结果被发送到第二个消息队列，然后我们在聚合数据库中更新结果。</li></ul><figure><img src="'+b+'" alt="recalculation-example" tabindex="0" loading="lazy"><figcaption>recalculation-example</figcaption></figure><h3 id="时间" tabindex="-1"><a class="header-anchor" href="#时间" aria-hidden="true">#</a> 时间</h3><p>我们需要时间戳来进行聚合。它可以在两个地方生成：</p><ul><li>事件时间 - 广告点击发生的时间</li><li>处理时间 - 系统处理事件时的时间</li></ul><p>由于使用了异步处理（消息队列）和网络延迟，事件时间和处理时间之间可能存在显著差异。</p><ul><li>如果我们使用处理时间，聚合结果可能不准确。</li><li>如果我们使用事件时间，我们必须处理延迟事件。</li></ul><p>没有完美的解决方案，我们需要权衡：</p><table><thead><tr><th></th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>事件时间</td><td>聚合结果更准确</td><td>客户端可能有错误的时间，或时间戳可能由恶意用户生成</td></tr><tr><td>处理时间</td><td>服务器时间戳更可靠</td><td>如果事件延迟，时间戳就不准确</td></tr></tbody></table><p>由于数据准确性非常重要，我们将使用事件时间进行聚合。</p><p>为了缓解延迟事件的问题，可以利用一种叫做“水印”的技术。</p><p>在下面的示例中，事件 2 错过了需要聚合的时间窗口：</p><figure><img src="'+y+'" alt="watermark-technique" tabindex="0" loading="lazy"><figcaption>watermark-technique</figcaption></figure><p>然而，如果我们故意扩展聚合窗口，就可以减少错过事件的可能性。 窗口扩展部分被称为“水印”：</p><figure><img src="'+x+'" alt="watermark-2" tabindex="0" loading="lazy"><figcaption>watermark-2</figcaption></figure><ul><li>短水印增加错过事件的可能性，但减少延迟。</li><li>长水印减少错过事件的可能性，但增加延迟。</li></ul><p>无论水印的大小如何，总是有错过事件的可能性。但对于这些低概率事件进行优化是没有意义的。</p><p>我们可以通过做每日结束时的对账来解决这些不一致问题。</p><h3 id="聚合窗口" tabindex="-1"><a class="header-anchor" href="#聚合窗口" aria-hidden="true">#</a> 聚合窗口</h3><p>有四种窗口函数：</p><ul><li>固定窗口（Tumbling Window）</li><li>滑动窗口（Hopping Window）</li><li>滑动窗口（Sliding Window）</li><li>会话窗口（Session Window）</li></ul><p>在我们的设计中，我们使用固定窗口（Tumbling Window）来进行广告点击聚合：</p><figure><img src="'+v+'" alt="tumbling-window" tabindex="0" loading="lazy"><figcaption>tumbling-window</figcaption></figure><p>以及使用滑动窗口来进行前 N 个点击广告的 M 分钟聚合：</p><figure><img src="'+k+'" alt="sliding-window" tabindex="0" loading="lazy"><figcaption>sliding-window</figcaption></figure><h3 id="投递保证" tabindex="-1"><a class="header-anchor" href="#投递保证" aria-hidden="true">#</a> 投递保证</h3><p>由于我们正在聚合的数据将用于计费，数据准确性是优先考虑的。</p><p>因此，我们需要讨论：</p><ul><li>如何避免处理重复事件</li><li>如何确保所有事件都已处理</li></ul><p>我们可以使用三种投递保证——至多一次（at-most-once），至少一次（at-least-once）和精确一次（exactly-once）。</p><p>在大多数情况下，如果少量重复是可以接受的，至少一次（at-least-once）就足够了。 但在我们的系统中，这种情况不适用，因为即使是小的差异也可能导致数百万美元的差距。 因此，我们需要使用精确一次的投递语义。</p><h3 id="数据去重" tabindex="-1"><a class="header-anchor" href="#数据去重" aria-hidden="true">#</a> 数据去重</h3><p>最常见的数据质量问题之一是重复数据。</p><p>重复数据可能来自多种来源：</p><ul><li>客户端 - 客户端可能会多次发送相同的事件。带有恶意意图的重复事件最好由风险引擎处理。</li><li>服务器故障 - 聚合服务节点在聚合过程中崩溃，且上游服务未收到确认，因此事件被重新发送。</li></ul><p>以下是由于未确认事件而发生数据重复的示例：</p><figure><img src="'+z+'" alt="data-duplication-example" tabindex="0" loading="lazy"><figcaption>data-duplication-example</figcaption></figure><p>在这个示例中，偏移量 100 将被多次处理并发送到下游。</p><p>一种尝试缓解这种情况的方法是将最后看到的偏移量存储在 HDFS/S3 中，但这样有可能导致结果永远无法到达下游：</p><figure><img src="'+w+'" alt="data-duplication-example-2" tabindex="0" loading="lazy"><figcaption>data-duplication-example-2</figcaption></figure><p>最终，我们可以在与下游交互时原子性地存储偏移量。为了实现这一点，我们需要实现分布式事务：</p><figure><img src="'+A+'" alt="data-duplication-example-3" tabindex="0" loading="lazy"><figcaption>data-duplication-example-3</figcaption></figure><p>个人备注：另外，如果下游系统以幂等方式处理聚合结果，那么就不需要分布式事务。</p><h3 id="扩展系统" tabindex="-1"><a class="header-anchor" href="#扩展系统" aria-hidden="true">#</a> 扩展系统</h3><p>让我们讨论一下系统如何在增长时进行扩展。</p><p>我们有三个独立的组件——消息队列、聚合服务和数据库。 由于它们是解耦的，我们可以独立扩展它们。</p><p>如何扩展消息队列：</p><ul><li>我们不对生产者设置限制，因此它们可以轻松扩展。</li><li>消费者可以通过将其分配到消费者组并增加消费者数量来进行扩展。</li><li>为了使其有效，我们还需要确保提前创建足够的分区。</li><li>此外，当有成千上万的消费者时，消费者重平衡可能需要一些时间，因此建议在非高峰时段进行。</li><li>我们还可以考虑按地理位置对主题进行分区，例如 <code>topic_na</code>、<code>topic_eu</code> 等。</li></ul><figure><img src="'+S+'" alt="scale-consumers" tabindex="0" loading="lazy"><figcaption>scale-consumers</figcaption></figure><p>如何扩展聚合服务：</p><figure><img src="'+P+'" alt="aggregation-service-scaling" tabindex="0" loading="lazy"><figcaption>aggregation-service-scaling</figcaption></figure><ul><li>Map-Reduce 节点可以通过增加更多节点轻松扩展。</li><li>聚合服务的吞吐量可以通过利用多线程进行扩展。</li><li>另外，我们可以利用 Apache YARN 等资源提供商来利用多进程。</li><li>选项 1 更简单，但选项 2 在实践中更常用，因为它更具可扩展性。</li><li>这是一个多线程示例：</li></ul><figure><img src="'+B+'" alt="multi-threading-example" tabindex="0" loading="lazy"><figcaption>multi-threading-example</figcaption></figure><p>如何扩展数据库：</p><ul><li>如果我们使用 Cassandra，它原生支持通过一致性哈希进行水平扩展。</li><li>如果向集群中添加新节点，数据会自动在所有（虚拟）节点之间重新平衡。</li><li>通过这种方法，无需手动（重新）分片。</li></ul><figure><img src="'+M+'" alt="cassandra-scalability" tabindex="0" loading="lazy"><figcaption>cassandra-scalability</figcaption></figure><p>另一个需要考虑的扩展性问题是热点问题——如果某个广告比其他广告更受欢迎，吸引更多注意力怎么办？</p><figure><img src="'+N+'" alt="hotspot-issue" tabindex="0" loading="lazy"><figcaption>hotspot-issue</figcaption></figure><ul><li>在上述示例中，聚合服务节点可以通过资源管理器申请额外资源。</li><li>资源管理器分配更多资源，因此原始节点不会过载。</li><li>原始节点将事件分为 3 组，每个聚合节点处理 100 个事件。</li><li>结果写回原始聚合节点。</li></ul><p>另一种更复杂的处理热点问题的方法：</p><ul><li>全局-局部聚合</li><li>分割独立聚合</li></ul><h3 id="故障容错" tabindex="-1"><a class="header-anchor" href="#故障容错" aria-hidden="true">#</a> 故障容错</h3><p>在聚合节点内，我们正在内存中处理数据。如果一个节点宕机，已处理的数据将丢失。</p><p>我们可以利用 Kafka 中的消费者偏移量来在其他节点接管时继续从中断处开始。 然而，由于我们正在聚合前 N 个广告，可能需要维护额外的中间状态。</p><p>我们可以在特定的分钟时进行快照，以便持续的聚合操作：</p><p>![</p><p>fault-tolerance-example](../image/system-design-309.png)</p><p>如果某个节点宕机，新节点可以读取最新的已提交的消费者偏移量和最新的快照，以继续执行任务：</p><figure><img src="'+G+'" alt="fault-tolerance-recovery-example" tabindex="0" loading="lazy"><figcaption>fault-tolerance-recovery-example</figcaption></figure><h3 id="数据监控和正确性" tabindex="-1"><a class="header-anchor" href="#数据监控和正确性" aria-hidden="true">#</a> 数据监控和正确性</h3><p>由于我们正在聚合的数据对于计费至关重要，因此确保正确性非常重要，必须实施严格的监控。</p><p>我们可能需要监控的一些指标：</p><ul><li>延迟 - 可以追踪不同事件的时间戳，以了解系统的端到端延迟。</li><li>消息队列大小 - 如果队列大小突然增加，我们需要增加更多的聚合节点。由于 Kafka 通过分布式提交日志实现，我们需要跟踪记录滞后指标。</li><li>聚合节点上的系统资源 - CPU、磁盘、JVM 等。</li></ul><p>我们还需要实现一个对账流程，这是一个批处理作业，在每天结束时运行。 它从原始数据计算聚合结果，并与聚合数据库中实际存储的数据进行比较：</p><figure><img src="'+T+'" alt="reconciliation-flow" tabindex="0" loading="lazy"><figcaption>reconciliation-flow</figcaption></figure><h3 id="替代设计" tabindex="-1"><a class="header-anchor" href="#替代设计" aria-hidden="true">#</a> 替代设计</h3><p>在通用的系统设计面试中，你不需要了解大数据处理中的专门软件的内部原理。</p><p>解释思考过程并讨论权衡比了解具体工具更为重要，这也是本章涵盖通用解决方案的原因。</p><p>一个替代设计，利用现成的工具，是将广告点击数据存储在 Hive 中，并在其上构建 ElasticSearch 层，以加速查询。</p><p>聚合通常在 OLAP 数据库中进行，如 ClickHouse 或 Druid。</p><figure><img src="'+C+'" alt="alternative-design" tabindex="0" loading="lazy"><figcaption>alternative-design</figcaption></figure><h2 id="第四步-总结" tabindex="-1"><a class="header-anchor" href="#第四步-总结" aria-hidden="true">#</a> 第四步：总结</h2><p>我们覆盖的内容：</p><ul><li>数据模型和 API 设计</li><li>使用 MapReduce 聚合广告点击事件</li><li>扩展消息队列、聚合服务和数据库</li><li>缓解热点问题</li><li>持续监控系统</li><li>使用对账确保正确性</li><li>故障容错</li></ul><p>广告点击事件聚合是一个典型的大数据处理系统。</p><p>如果你事先了解相关技术，那么理解和设计它将变得更加容易：</p><ul><li>Apache Kafka</li><li>Apache Spark</li><li>Apache Flink</li></ul>',107);function H(q,j){return d(),l("div",null,[U,i(" prettier-ignore "),I,i(" prettier-ignore "),F,i(" prettier-ignore "),K,i(" prettier-ignore "),O,i(" prettier-ignore "),W,D,i(" prettier-ignore "),L,Y,E,i(" prettier-ignore "),Q,i(" prettier-ignore "),V])}const Z=a(R,[["render",H],["__file","41_ad_click_event_aggregation.html.vue"]]);export{Z as default};
